---
title: "Statistics - Assignment 4"
author: Dawid Wegner
date: 28/11/2020
output: html_notebook
---

```{r}
set.seed(100)
```

```{r}
get_distributions_colnames <- function () {
  return(c(
    "norm_mean0_sd1", "norm_mean1_sd1", "norm_mean0_sd2", "norm_mean1_sd2", "logis_mean0_scale1", "logis_mean1_scale1",
    "logis_mean0_scale2", "logis_mean1_scale2", "cauchy_mean0_scale1", "cauchy_mean1_scale1", "cauchy_mean0_scale2",
    "cauchy_mean1_scale2"
  ))
}

get_experiments_pairs <- function () {
  experiments_df <- data.frame(matrix(nrow = 12, ncol = 2))
  colnames(experiments_df) <- c("distribution1", "distribution2")
  experiments_df["distribution1"] <- c(
    "norm_mean0_sd1", "norm_mean0_sd1", "norm_mean0_sd1", "norm_mean0_sd1", "logis_mean0_scale1", "logis_mean0_scale1",
    "logis_mean0_scale1", "logis_mean0_scale1", "cauchy_mean0_scale1", "cauchy_mean0_scale1", "cauchy_mean0_scale1",
    "cauchy_mean0_scale1"
  )
  experiments_df["distribution2"] <- c(
    "norm_mean0_sd1", "norm_mean1_sd1", "norm_mean0_sd2", "norm_mean1_sd2", "logis_mean0_scale1", "logis_mean1_scale1",
    "logis_mean0_scale2", "logis_mean1_scale2", "cauchy_mean0_scale1", "cauchy_mean1_scale1", "cauchy_mean0_scale2",
    "cauchy_mean1_scale2"
  )
  return(experiments_df)
}

get_distributions_means <- function () {
  means_df <- data.frame(matrix(nrow = 1, ncol = 12))
  colnames(means_df) <- get_distributions_colnames()
  means_df["norm_mean0_sd1"] <- 0.0
  means_df["norm_mean1_sd1"] <- 1.0
  means_df["norm_mean0_sd2"] <- 0.0
  means_df["norm_mean1_sd2"] <- 1.0
  means_df["logis_mean0_scale1"] <- 0.0
  means_df["logis_mean1_scale1"] <- 1.0
  means_df["logis_mean0_scale2"] <- 0.0
  means_df["logis_mean1_scale2"] <- 1.0
  means_df["cauchy_mean0_scale1"] <- 0.0
  means_df["cauchy_mean1_scale1"] <- 1.0
  means_df["cauchy_mean0_scale2"] <- 0.0
  means_df["cauchy_mean1_scale2"] <- 1.0
  return(means_df)
}

get_distributions_variances <- function () {
  vars_df <- data.frame(matrix(nrow = 1, ncol = 12))
  colnames(vars_df) <- get_distributions_colnames()
  vars_df["norm_mean0_sd1"] <- 1.0
  vars_df["norm_mean1_sd1"] <- 1.0
  vars_df["norm_mean0_sd2"] <- 4.0
  vars_df["norm_mean1_sd2"] <- 4.0
  vars_df["logis_mean0_scale1"] <- pi ^ 2 / 3.0
  vars_df["logis_mean1_scale1"] <- pi ^ 2 / 3.0
  vars_df["logis_mean0_scale2"] <- 4.0 * pi ^ 2 / 3.0
  vars_df["logis_mean1_scale2"] <- 4.0 * pi ^ 2 / 3.0
  vars_df["cauchy_mean0_scale1"] <- 1.0
  vars_df["cauchy_mean1_scale1"] <- 1.0
  vars_df["cauchy_mean0_scale2"] <- 4.0
  vars_df["cauchy_mean1_scale2"] <- 4.0
  return(vars_df)
}

generate_distributions_samples <- function (samples_count) {
  samples_df <- data.frame(matrix(nrow = samples_count, ncol = 12))
  colnames(samples_df) <- get_distributions_colnames()
  samples_df["norm_mean0_sd1"] <- rnorm(n = samples_count, mean = 0, sd = 1)
  samples_df["norm_mean1_sd1"] <- rnorm(n = samples_count, mean = 1, sd = 1)
  samples_df["norm_mean0_sd2"] <- rnorm(n = samples_count, mean = 0, sd = 2)
  samples_df["norm_mean1_sd2"] <- rnorm(n = samples_count, mean = 1, sd = 2)
  samples_df["logis_mean0_scale1"] <- rlogis(n = samples_count, location = 0, scale = 1)
  samples_df["logis_mean1_scale1"] <- rlogis(n = samples_count, location = 1, scale = 1)
  samples_df["logis_mean0_scale2"] <- rlogis(n = samples_count, location = 0, scale = 2)
  samples_df["logis_mean1_scale2"] <- rlogis(n = samples_count, location = 1, scale = 2)
  samples_df["cauchy_mean0_scale1"] <- rcauchy(n = samples_count, location = 0, scale = 1)
  samples_df["cauchy_mean1_scale1"] <- rcauchy(n = samples_count, location = 1, scale = 1)
  samples_df["cauchy_mean0_scale2"] <- rcauchy(n = samples_count, location = 0, scale = 2)
  samples_df["cauchy_mean1_scale2"] <- rcauchy(n = samples_count, location = 1, scale = 2)
  return(samples_df)
}

aggregate_means_diff <- function (mean1, mean2) {
  return(mean1 - mean2)
}

aggreagate_vars_quotient <- function (variance1, variance2) {
  return(variance2 / variance1)
}

simulate_coverage_frequency <- function (
  parameters_df, parameters_aggregation_func, interval_func, samples_count = 50, simulations_count = 10000
) {
  experiments_df <- get_experiments_pairs()
  experiment_names <- with(experiments_df, paste(distribution1, distribution2, sep=" "))
  coverage_df <- data.frame(matrix(data = 0, nrow = 1, ncol = dim(experiments_df)[1]))
  colnames(coverage_df) <- experiment_names
  samples_df1 <- generate_distributions_samples(simulations_count * samples_count)
  samples_df2 <- generate_distributions_samples(simulations_count * samples_count)
  for (i in 1:simulations_count) {
    min_sample_index <- samples_count * (i - 1) + 1
    max_sample_index <- samples_count * i
    samples_slice1 <- samples_df1[min_sample_index:max_sample_index, ]
    samples_slice2 <- samples_df2[min_sample_index:max_sample_index, ]
    for (exp_index in 1:dim(experiments_df)[1]) {
      distribution1 <- experiments_df[exp_index, "distribution1"]
      distribution2 <- experiments_df[exp_index, "distribution2"]
      samples1 <- samples_slice1[[distribution1]]
      samples2 <- samples_slice2[[distribution2]]
      real_value <- parameters_aggregation_func(c(parameters_df[[distribution1]]), c(parameters_df[[distribution2]]))
      estimated_interval <- interval_func(distribution1, distribution2, samples1, samples2)
      experiment_name <- paste(distribution1, distribution2, sep=" ")
      if (is.na(estimated_interval[1])) {
        coverage_df[experiment_name] <- NA
      } else if (estimated_interval[1] <= real_value && estimated_interval[2] >= real_value) {
        coverage_df[experiment_name] <- coverage_df[experiment_name] + 1
      }
    }
  }
  coverage_df <- coverage_df / simulations_count
  return(coverage_df[, colSums(is.na(coverage_df)) == 0])
}

means_df <- get_distributions_means()
vars_df <- get_distributions_variances()
```


# Task 1

The goal is to find the distribution of the random variable $\bar{D} = \bar{X} - \bar{Y} = \frac{1}{n}\sum_k X_k - \frac{1}{n}\sum_k Y_k$, where $X_k \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $Y_k \sim \mathcal{N}(\mu_2, \sigma_2^2)$. From the properties of the expected value and variance, it follows that the $\bar{D}$ random variable has the $\mathcal{N}(\mu_1 - \mu_2, \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2})$ distribution. Thus, the variable $Z = \frac{\bar{D} - (\mu_1 - \mu_2)}{\sigma_1^2 / n_1 + \sigma_2^2 / n_2}$ has the standard normal distribution. From the quantiles $z_{\alpha/2}$, $z_{1 - \alpha/2}$ of the $Z$ distribution, it follows that the $100(1 - \alpha)\%$ confidence interval of the standard normal distribution is equal to $z_{\alpha/2} \leq Z \leq z_{1 - \alpha/2}$. After substituting our expression for $Z$, we get the confidence interval for the mean of our variable: $(\bar{X} - \bar{Y}) - z_{1 - \alpha/2}\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \leq \mu_1 - \mu_2 \leq (\bar{X} - \bar{Y}) + z_{1 - \alpha/2} \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$.


# Task 2

```{r}
get_means_diff_ci <- function (distribution1, distribution2, samples1, samples2) {
  diff_mean <- mean(samples1) - mean(samples2)
  variance1 <- vars_df[[distribution1]]
  variance2 <- vars_df[[distribution2]]
  diff_sd <- sqrt(variance1 / length(samples1) + variance2 / length(samples2))
  quantile <- qnorm(0.975)
  return(c(diff_mean - quantile * diff_sd, diff_mean + quantile * diff_sd))
}
```

```{r}
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_ci)
```

The results of the experiments are in line with the theory. Firstly, the coverage probabilities of the normal distributions are close to the significance level of $95\%$. The small errors are expected as the finite number of $10000$ simulations was performed. The coverage probabilities of the logistic distributions are also close to the significance level. This phenomenon is expected as it follows from the central limit theorem that the distribution of the mean of independent random variables from the same distribution converges to the normal distribution. In practice, for distributions with low skewness it suffices to have about $30$ samples. In particular, the skewness of the logistic distribution is equal to $0$. Lastly, the coverage probabilities of the Cauchy distribution are far away from the significance level. Regardless of the parameters, the expected value of this distribution is undefined. Additionally, the variance of this distribution is undefined, implying that our confidence intervals are strongly disturbed. These properties explain the observed results.


# Task 3

The theory is similar to the theory of finding a mean of one variable with the unknown variance. Using the fact that the variances of $\bar{X}, \bar{Y}$ variables are the same, we can estimate the variance of the variable $\bar{D} = \bar{X} - \bar{Y}$ as $S_D^2 = \frac{(n_1 - 1)S_x^2 + (n_2 - 1)S_y^2}{n_1 + n_2 - 2}$, where $n_1, n_2$ represent the number of samples of the $X$ variable and the number of samples of the $Y$ variable, respectively. The distribution of the $T = \frac{\bar{X} - \bar{Y} - (\mu_1 - \mu_2)}{\sqrt{S_D^2 / n_1 + S_D^2 / n_2}}$ variable is known as a Student's t-distribution. By following the analogous steps as in the case when the variance was known, we get the following confidence interval: $(\bar{X} - \bar{Y}) - t_{1 - \alpha/2}\sqrt{S_D^2 / n_1 + S_D^2 / n_2} \leq \mu_1 - \mu_2 \leq (\bar{X} - \bar{Y}) + t_{1 - \alpha/2}\sqrt{S_D^2 / n_1 + S_D^2 / n_2}$, where $t_{1 - \alpha/2}$ is the quantile of the Student's t-distribution with $n_1 + n_2 - 2$ degrees of freedom.


# Task 4

```{r}
get_means_diff_unknown_eq_sd_ci <- function (distribution1, distribution2, samples1, samples2) {
  diff_mean <- mean(samples1) - mean(samples2)
  df1 <- length(samples1) - 1
  df2 <- length(samples2) - 1
  joint_df <- df1 + df2
  joint_var <- (df1 * var(samples1) + df2 * var(samples2)) / joint_df
  quantile <- qt(0.975, df = joint_df)
  diff_sd <- sqrt(joint_var * (1 / length(samples1) + 1 / length(samples2)))
  return(c(diff_mean - quantile * diff_sd, diff_mean + quantile * diff_sd))
}
```

```{r}
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_eq_sd_ci)
```

The simulated coverage probability is close to the significance level in the cases of the normal and logistic distributions. For the normal distribution these results were expected from the theory of estimating such confidence intervals. In the case of the logistic distribution, the central limit theorem is applicable, thus the results are comparable to those when the normal distribution is used. Surprisingly, the estimations are also good when the variances of the variables are not equal. It can be the result of the fact that the number of degrees of freedom is pretty high, so the t-distribution is close to the normal distribution.

  In contrast, the errors for the Cauchy distribution are significant. This can be the effect of the property that the mean of the Cauchy distribution is undefined. Additionally, the errors for the Cauchy distribution are smaller compared to the experiment in which the variance was known. This can be the effect of the fact that the variance of the Cauchy distribution is unknown, thus using a scale parameter as a variance to estimate confidence intervals can give poorer results than using a variance of samples.


# Task 5

The variance of the variable $\bar{D} = \bar{X} - \bar{Y}$ is equal to $S_D^2 = \frac{S_x^2}{n_x} + \frac{S_y^2}{n_y}$ and can be estimated using variances of the variables $X, Y$ the basic properties of variance. Similarly to previous cases, the variable $T = \frac{\bar{X} - \bar{Y}}{S_x^2 / n_x + S_y^2 / n_y}$ has the student's t-distribution. The only unknown quantity is the number of degrees of freedom $df$. Fortunately, it can be approximated using the Welch–Satterthwaite equation that states that $df \approx \frac{(S_x^2 / n_x + S_y^2 / n_y)^2}{S_x^4 / n_x^2(n_x - 1) + S_y^4 / n_y^2(n_y - 1)}$. Thus, after following the same steps as in the previous cases, we get the following confidence interval: $(\bar{X} - \bar{Y}) - t_{1 - \alpha/2}\sqrt{S_x^2 / n_x + S_y^2 / n_y} \leq \mu_1 - \mu_2 \leq (\bar{X} - \bar{Y}) + t_{1 - \alpha/2}\sqrt{S_x^2 / n_x + S_y^2 / n_y}$, where $t_{1 - \alpha/2}$ represents the quantile of the student's t-distribution with $df$ degrees of freedom.


# Task 6

```{r}
get_means_diff_unknown_neq_sd_ci <- function (distribution1, distribution2, samples1, samples2) {
  diff_mean <- mean(samples1) - mean(samples2)
  n1 <- length(samples1)
  n2 <- length(samples2)
  diff_sd <- sqrt(var(samples1) / n1 + var(samples2) / n2)
  df_enumerator <- (var(samples1) / n1 + var(samples2) / n2) ^ 2
  df_denominator <- var(samples1) ^ 2 / (n1 ^ 2 * (n1 - 1)) + var(samples2) ^ 2 / (n2 ^ 2 * (n2 - 1))
  joint_df <- ceiling(df_enumerator / df_denominator)
  quantile <- qt(0.975, df = joint_df)
  return(c(diff_mean - quantile * diff_sd, diff_mean + quantile * diff_sd))
}
```

```{r}
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_neq_sd_ci)
```

The results are very close to the results when the variances of the variables were equal. Similarly, the simulated coverage probability is close to the significance level when the normal and logistic distributions are used, while it is far away from the significance level when the Cauchy distribution is used. In the cases when the variances are not equal, the results are slightly better compared to the experiment in which it was assumed that the variances are equal. But the differences are so small that even the impact of using the finite number of simulations is higher. Perhaps, the difference could be more visible if the number of samples would be much lower.


# Task 7

Our goal is to estimate the confidence interval of the quantity $\frac{\sigma_y^2}{\sigma_x^2}$, where $\sigma_x^2$ is the variance of the variables $X_k \sim \mathcal{N}(\mu_x, \sigma_x^2)$ and $\sigma_y^2$ is the variance of the variables $Y_k \sim \mathcal{N}(\mu_y, \sigma_y^2)$. As the variables $\mu_x, \mu_y$ are known, we can estimate the variances as $S_x^2 = \frac{1}{n_x} \sum_k (X_k - \mu_x)^2$ and $S_y^2 = \frac{1}{n_y} \sum_k (Y_k - \mu_y)^2$. The distribution of the quantity $\frac{S_y^2}{S_x^2}$ can be estimated using the fact that the distributions of the variables $\frac{n_x S_x^2}{\sigma_x^2}$, $\frac{n_y S_y^2}{\sigma_y^2}$ are known as chi-square distributions with $n_x$, $n_y$ degrees of freedom, respectively. What is more, the distribution of the variable $\frac{\sigma_y^2 S_x^2}{\sigma_x^2 S_y^2}$ is known as the Fisher distribution with $n_x, n_y$ degrees of freedom. It implies that the confidence interval is equal to $F_1 \frac{S_y^2}{S_x^2} \leq \frac{\sigma_y^2}{\sigma_x^2} \leq F_2 \frac{S_y^2}{S_x^2}$, where $F_1$ represents the $\frac{\alpha}{2}$ quantile of the Fisher distribution with $n_x, n_y$ degrees of freedom and $F_2$ represents the $1 - \frac{\alpha}{2}$ quantile of the Fisher distribution with $n_x, n_y$ degrees of freedom.

# Task 8

```{r}
get_vars_quotient_ci <- function (distribution1, distribution2, samples1, samples2) {
  var1 <- 1 / length(samples1) * sum((samples1 - mean(samples1)) ^ 2)
  var2 <- 1 / length(samples2) * sum((samples2 - mean(samples2)) ^ 2)
  vars_quotient <- var2 / var1
  quantile1 <- qf(0.025, df1 = length(samples1), df2 = length(samples2))
  quantile2 <- qf(0.975, df1 = length(samples1), df2 = length(samples2))
  return(c(quantile1 * vars_quotient, quantile2 * vars_quotient))
}
```

```{r}
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_ci)
```

The first outcome from the experiment is that the confidence interval of the normal distribution is approximated very well. This is not true for the other distributions. From the theoretical perspective, these distributions are not guaranteed to converge to the Fisher distribution as the central limit theorem doesn't apply in this case. The experiment proves that there exist some distributions that actually don't converge to the normal distribution, at least when $50$ samples are used. Additionally, the results for the Cauchy distribution are very poor as the mean and variance of this distribution are undefined.


# Task 9

The theory is similar to the theory of estimating the quantity $\frac{\sigma_y^2}{\sigma_x^2}$ when the $\mu_x, \mu_y$ were known. The main difference is the estimation of the variances that are equal to $S_x^2 = \frac{1}{n_x - 1} \sum_k (X_k - \mu_x)^2$ and $S_y^2 = \frac{1}{n_y - 1} \sum_k (Y_k - \mu_y)^2$ in this case. As a result, the distribution of the quantity $\frac{\sigma_y^2 S_x^2}{\sigma_x^2 S_y^2}$ is the Fisher distribution with $n_x - 1$, $n_y - 1$ degrees of freedom. It implies that the confidence interval is equal to $F_1 \frac{S_y^2}{S_x^2} \leq \frac{\sigma_y^2}{\sigma_x^2} \leq F_2 \frac{S_y^2}{S_x^2}$, where $F_1$ represents the $\frac{\alpha}{2}$ quantile of the Fisher distribution with $n_x - 1, n_y - 1$ degrees of freedom and $F_2$ represents the $1 - \frac{\alpha}{2}$ quantile of the Fisher distribution with $n_x - 1, n_y - 1$ degrees of freedom.


# Task 10

```{r}
get_vars_quotient_uknown_sd_ci <- function (distribution1, distribution2, samples1, samples2) {
  var1 <- var(samples1)
  var2 <- var(samples2)
  vars_quotient <- var2 / var1
  quantile1 <- qf(0.025, df1 = length(samples1) - 1, df2 = length(samples2) - 1)
  quantile2 <- qf(0.975, df1 = length(samples1) - 1, df2 = length(samples2) - 1)
  return(c(quantile1 * vars_quotient, quantile2 * vars_quotient))
}
```

```{r}
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_uknown_sd_ci)
```

Similarly to the previous experiment, the confidence intervals of the normal distribution are approximated very well. The accuracy of approximations is similar to the accuracy of approximations when the $\mu_x, \mu_y$ were known. In contrast, the confidence intervals are not approximated well when logistic distribution and Cauchy distribution are used. While in the case of the logistic distribution the cause of poor results is that the central limit theorem doesn't apply in this case, in the case of the Cauchy distribution it is caused by the fact that its variance is undefined.


# Task 11

```{r}
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_ci, samples_count = 20)
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_eq_sd_ci, samples_count = 20)
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_neq_sd_ci, samples_count = 20)
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_ci, samples_count = 20)
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_uknown_sd_ci, samples_count = 20)
```

```{r}
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_ci, samples_count = 100)
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_eq_sd_ci, samples_count = 100)
simulate_coverage_frequency(means_df, aggregate_means_diff, get_means_diff_unknown_neq_sd_ci, samples_count = 100)
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_ci, samples_count = 100)
simulate_coverage_frequency(vars_df, aggreagate_vars_quotient, get_vars_quotient_uknown_sd_ci, samples_count = 100)
```

The experiments show that the approximations of confidence intervals are a bit worse when the number of samples is decreased to $20$. In contrast, the approximations are better when $100$ samples are used. The differences are especially visible for the logistic distribution in the experiments that approximate a quotient of variances. Since the central limit theorem doesn't apply in this case, such significant differences were not expected. Thus, it can be a coincidence. Lastly, the number of samples doesn't impact the results for the Cauchy distribution. This is expected as the general problem with this distribution is that its mean and variance are undefined.


# Task 12

The delta method can be used to approximate the variance of the transformed distribution. In more detail, the theorem states that if $\sqrt{n}(X_n - \theta) \xrightarrow{D} \mathcal{N}(0, \sigma^2)$, then $\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{D} \mathcal{N}(0, \sigma^2 (g'(\theta)^2))$, where $g$ is differentiable function that represents a transformation and $g'(\theta) \neq 0$. Thus, the most basic application of the method would be to derive the variance of a distribution $D_t$ that is a transformation of another distribution $D_s$ with a known variance. Then, this variance can be used to calculate the confidence interval of a mean for the $D_t$ distribution.

The other application of the delta method would be to calculate the confidence interval of the transformation of a parameter $\theta$ with the estimated Fisher information $I(\hat{\theta})$. The confidence interval for the $g(\theta)$ follows directly from the following theorem: $\sqrt{n}(g(\hat{\theta_n}) - g(\theta_0)) \xrightarrow{D} \mathcal{N}(0, \frac{g'(\theta_0)^2}{I(\theta_0)})$.
